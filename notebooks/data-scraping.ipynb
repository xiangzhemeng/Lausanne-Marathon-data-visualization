{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "% matplotlib inline\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load data from pickle\n"
     ]
    }
   ],
   "source": [
    "rankings_p = '../data/rankings.p'\n",
    "headers_p = '../data/headers.p'\n",
    "\n",
    "# function to scrape marathon data\n",
    "def scrape_marathon_data():\n",
    "    years = range(2002, 2018)\n",
    "    letters = [chr(x + ord('A')) for x in range(0, 26)]\n",
    "\n",
    "    URL = \"https://services.datasport.com/{}/lauf/lamara/ALFA{}.HTM\"\n",
    "    \n",
    "    rankings = {}\n",
    "    headers = {}\n",
    "    for year in years:\n",
    "        print(str(year) + \": \", end='')\n",
    "        rankings[str(year)] = []\n",
    "        for letter in letters:\n",
    "            r = requests.get(URL.format(year, letter))\n",
    "            soup = bs(r.text, 'html.parser')\n",
    "            title = soup.find('title').getText()\n",
    "            if(title != 'Adresse nicht vorhanden / The address is not available'):\n",
    "                data = soup.findAll('font', {'size':'2'})\n",
    "\n",
    "                print(letter, end='')\n",
    "                headers[str(year)] = data[0].getText()\n",
    "                rankings[str(year)].extend(re.compile('¦ *\\r\\n').split(data[1].getText()))\n",
    "            else:\n",
    "                print(\"Skipped\")\n",
    "        print('')\n",
    "\n",
    "    # Pickle data\n",
    "    pickle.dump(rankings, open(rankings_p, 'wb'))\n",
    "    pickle.dump(headers, open(headers_p, 'wb'))\n",
    "    return rankings, headers\n",
    "\n",
    "# Only scrape if pickle unavailable\n",
    "def get_data(force_scrape=False):\n",
    "    if force_scrape:\n",
    "        print('Scrape data from website')\n",
    "        return scrape_marathon_data()\n",
    "    try:\n",
    "        print('Trying to load data from pickle')\n",
    "        return pickle.load(open(rankings_p, 'rb')), pickle.load(open(headers_p, 'rb'))\n",
    "    except (OSError, IOError) as e:\n",
    "        print('Failed to load pickle:')\n",
    "        print(e)\n",
    "        print('Scrape data from website')\n",
    "        return scrape_marathon_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function finds the position of where the pages should be split into columns\n",
    "based on whitespace\n",
    "\"\"\"\n",
    "def parse_columns(ranking, header):\n",
    "    column_structure = {}\n",
    "    column_mask = np.ones(np.array(list(ranking[0])).shape, dtype=bool)\n",
    "    spaces_mask = np.ones(np.array(list(ranking[0])).shape)*32\n",
    "    \n",
    "    # create mask of space positions for field delimiter detection\n",
    "    for line in ranking[:-1]:\n",
    "        cur_line = np.array(list(line)).view(np.uint32)\n",
    "        \n",
    "        try:\n",
    "            column_mask = np.logical_and(column_mask, np.equal(cur_line, spaces_mask))\n",
    "        except:\n",
    "            #print(\"Skipped line:\", line)\n",
    "            #print(cur_line.shape, column_mask.shape)\n",
    "            pass\n",
    "        \n",
    "    # include header in space position mask\n",
    "    header_int = np.array(list(header[:column_mask.shape[0]])).view(np.uint32)\n",
    "    column_mask = np.logical_and(column_mask, np.equal(header_int, spaces_mask))\n",
    "        \n",
    "    # find field delimiter positions\n",
    "    inside_space_col = False\n",
    "    delimiters = []\n",
    "    for i, is_space_col in enumerate(list(column_mask)):\n",
    "        if is_space_col:\n",
    "            if not inside_space_col:\n",
    "                inside_space_col = True\n",
    "                delimiters.append(i)\n",
    "                \n",
    "        elif inside_space_col:\n",
    "            inside_space_col = False\n",
    "            \n",
    "    return delimiters\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "Seperates the lists of lines into columns and create a dictionary of dataframes\n",
    "\"\"\"\n",
    "def separate_cols(rankings, headers):\n",
    "    # dictionary to keep dataframes in\n",
    "    dfs = {}\n",
    "    for year in rankings:\n",
    "        ranking = rankings[year]\n",
    "        header = headers[year]\n",
    "        \n",
    "        # fix offset headers in 2007-2009 data\n",
    "        if year in ['2007', '2008', '2009']:\n",
    "            header = header.replace('an lieu', 'an   lieu')\n",
    "        delimiters = parse_columns(ranking, header)\n",
    "\n",
    "        # find positions for each field\n",
    "        start = 0\n",
    "        cols = {} \n",
    "        unnamed = 0\n",
    "        for delimiter in delimiters:\n",
    "            key = header[start:delimiter+1].strip()\n",
    "            if key == '':\n",
    "                key = unnamed\n",
    "                unnamed += 1\n",
    "            cols[key] = (start, delimiter+1)\n",
    "            start = delimiter+1\n",
    "\n",
    "        start = 0\n",
    "        result = {}\n",
    "        for i, line in enumerate(ranking):\n",
    "            if '\\r\\ntotal ' not in line and '\\r\\nTotal' not in line:\n",
    "                for col in cols:\n",
    "                    start, end = cols[col]\n",
    "                    if col in result:\n",
    "                        result[col].append(line[start:end].strip())\n",
    "                    else:\n",
    "                        result[col] = [line[start:end].strip()]\n",
    "        dfs[year] = pd.DataFrame(result, columns=result.keys()) \n",
    "    print('Columns separated')\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    'lieu': '',\n",
    "    0: 'DELETE',\n",
    "    2: 'DELETE',\n",
    "    3: 'DELETE',\n",
    "    'catégorie': '',\n",
    "    'nom': '',\n",
    "    1: 'DELETE',\n",
    "    'temps': '',\n",
    "    'Jg': 'an',\n",
    "    'an lieu': '',\n",
    "    'overall': '',\n",
    "    'retard': '',\n",
    "    'équipe': '',\n",
    "    'moyenne': '',\n",
    "    'Rang': 'rang',\n",
    "    'nom/lieu': '',\n",
    "    'Stnr': 'doss',\n",
    "    'an': '',\n",
    "    'pénalité': '',\n",
    "    'doss': '',\n",
    "    'temps net': '',\n",
    "    'rang': '',\n",
    "    '¦': 'DELETE',\n",
    "    'équipe/lieu': '',\n",
    "    'Rückstand': 'retard',\n",
    "    'Kategorie': 'catégorie',\n",
    "    'Team/Ortschaft': 'équipe/lieu',\n",
    "    'Name/Ort': 'nom/lieu',\n",
    "    'Zeit': 'temps'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Where possible, this function matches the names of the columns \n",
    "of the individual dataframes\n",
    "\"\"\"\n",
    "def normalize_column_names(dfs_):\n",
    "    dfs_c = dfs_.copy()\n",
    "    for year, _ in dfs_c.items():\n",
    "        for header_name in list(dfs_c[year]):\n",
    "            if mapping[header_name] == 'DELETE':\n",
    "                dfs_c[year] = dfs_c[year].drop(header_name, axis=1)\n",
    "            elif mapping[header_name] != \"\":\n",
    "                dfs_c[year] = dfs_c[year].rename(columns = {header_name : mapping[header_name]})\n",
    "                print(\"Renamed {} to {} in year {}\".format(\n",
    "                    header_name, mapping[header_name], year))\n",
    "    print('Column names normalized')\n",
    "    return dfs_c\n",
    "                \n",
    "\n",
    "\n",
    "def print_col_overview(dfs):\n",
    "    # fields present in all years\n",
    "    shared_fields = set.intersection(*[set(v) for k, v in dfs.items()])\n",
    "    print(\"Shared fields:\", shared_fields)\n",
    "\n",
    "    fields = set()\n",
    "    for year in sorted(dfs):\n",
    "        fields = fields.union(set(list(dfs[year])))\n",
    "        print(year)\n",
    "        print(set(dfs[year]).difference(shared_fields))\n",
    "    print(\"Union remaining fields:\", fields.difference(shared_fields))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_cities(dfs_):\n",
    "    dfs_c = dfs_.copy()\n",
    "    \"\"\"\n",
    "    Years 2002-2006 have a team/city field and a name/city field.\n",
    "    If a team is provided, the city is added to the end of the name,\n",
    "    after a comma (',').\n",
    "    \n",
    "    This function removes the teams and converts the column 'équipe/lieu' \n",
    "    to a dedicated 'lieu' column\n",
    "    \"\"\"\n",
    "    for year, df in dfs_c.items():\n",
    "        col_name = 'nom/lieu'\n",
    "        col_team = 'équipe/lieu'\n",
    "        if col_name in df:\n",
    "            for i, row in df.iterrows():\n",
    "                m = re.compile('(.*), (.*)').match(row[col_name])\n",
    "                if m is not None:\n",
    "                    dfs_c[year].at[i, col_name] = m.group(1)\n",
    "                    dfs_c[year].at[i, col_team] = m.group(2)\n",
    "            # rename columns\n",
    "        rename_dict = {col_name : 'nom', col_team: 'lieu'}\n",
    "        dfs_c[year].rename(columns=rename_dict, inplace=True)\n",
    "    print('Cities fixed')\n",
    "    return dfs_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Returns the fields that are present in all years\n",
    "\"\"\"\n",
    "def get_shared_fields(dfs):\n",
    "    return set.intersection(*[set(v) for k, v in dfs.items()])\n",
    "\n",
    "\"\"\"\n",
    "Returns the fields that are only present in some of the years\n",
    "\"\"\"\n",
    "def get_unshared_fields(dfs):\n",
    "    return set.union(*[set(v) for k, v in dfs.items()]).difference(get_shared_fields(dfs))\n",
    "\n",
    "\"\"\"\n",
    "Deletes columns from dataframes that are only present in some of the years\n",
    "\"\"\"\n",
    "def drop_uncommon_fields(dfs_):\n",
    "    dfs_c = dfs_.copy()\n",
    "    for year, df in dfs_c.items():\n",
    "        dfs_c[year] = dfs_c[year][list(get_shared_fields(dfs_c))]\n",
    "    print('Uncommon fields dropped')\n",
    "    return dfs_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_year_column(dfs_):\n",
    "    dfs_c = dfs_.copy()\n",
    "    for year in dfs_c:\n",
    "        df = dfs_c[year].copy()\n",
    "        df['race-year'] = int(year)\n",
    "        dfs_c[year] = df\n",
    "    print('Year column created')\n",
    "    return dfs_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_merged_df(dfs_):\n",
    "    return pd.concat(list(dfs_.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_categories(df_):\n",
    "    \"\"\"\n",
    "    rename important categories and drop unused\n",
    "    \"\"\"\n",
    "    df_c = df_.copy()\n",
    "    df_c['catégorie'] = df_['catégorie'].replace({\n",
    "        r'^(\\d\\d)[-/][MQS]*([DH])\\d+$' : r'\\1|\\2',\n",
    "        r'^Q([DH])\\d$' : r'10|\\1',\n",
    "        r'^S([DH])\\d$' : r'21|\\1',\n",
    "        r'^M([DH])\\d$' : r'42|\\1'\n",
    "    }, regex = True)\n",
    "    \n",
    "    # columns to keep\n",
    "    keep = [str(x) + '|H' for x in [10, 21, 42]]\n",
    "    keep.extend([str(x) + '|D' for x in [10, 21, 42]])\n",
    "    \n",
    "    # drop all other rows\n",
    "    df_c = df_c[df_c['catégorie'].isin(keep)]\n",
    "    \n",
    "    # split catégorie into two columns\n",
    "    df_c[['distance', 'gender']] = df_c['catégorie'].str.split(\n",
    "        '|', expand=True\n",
    "    )\n",
    "    \n",
    "    # replace german gender abbreviations\n",
    "    df_c['gender'] = df_c['gender'].replace({\n",
    "        'H' : 'male',\n",
    "        'D' : 'female'\n",
    "    })\n",
    "    \n",
    "    df_c = df_c.drop('catégorie', axis=1)\n",
    "    \n",
    "    return df_c\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load data from pickle\n",
      "Columns separated\n",
      "Renamed Stnr to doss in year 2002\n",
      "Renamed Kategorie to catégorie in year 2002\n",
      "Renamed Name/Ort to nom/lieu in year 2002\n",
      "Renamed Zeit to temps in year 2002\n",
      "Renamed Rang to rang in year 2002\n",
      "Renamed Jg to an in year 2002\n",
      "Renamed Team/Ortschaft to équipe/lieu in year 2002\n",
      "Renamed Rückstand to retard in year 2002\n",
      "Column names normalized\n",
      "Cities fixed\n",
      "Uncommon fields dropped\n",
      "Year column created\n"
     ]
    }
   ],
   "source": [
    "# Get the data\n",
    "rankings, headers = get_data()\n",
    "# Separate data into columns and build dataframes\n",
    "dfs = separate_cols(rankings, headers)\n",
    "# Make sure the same columns have the same column names\n",
    "dfs_n = normalize_column_names(dfs)\n",
    "# Fix cities which are in the name column in some cases\n",
    "dfs_f = fix_cities(dfs_n)\n",
    "# Drop columns that are unique to some years\n",
    "dfs_d = drop_uncommon_fields(dfs_f)\n",
    "# Create year columns\n",
    "dfs_y = create_year_column(dfs_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final dataframe\n",
    "df = create_merged_df(dfs_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lieu</th>\n",
       "      <th>catégorie</th>\n",
       "      <th>rang</th>\n",
       "      <th>temps</th>\n",
       "      <th>an</th>\n",
       "      <th>retard</th>\n",
       "      <th>nom</th>\n",
       "      <th>doss</th>\n",
       "      <th>lieu</th>\n",
       "      <th>temps</th>\n",
       "      <th>rang</th>\n",
       "      <th>race-year</th>\n",
       "      <th>distance</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N-Oteren</td>\n",
       "      <td>10-H30</td>\n",
       "      <td>676.</td>\n",
       "      <td>1:15.48,7</td>\n",
       "      <td>1981</td>\n",
       "      <td>Aarskog Kay-Morten</td>\n",
       "      <td>(16798)</td>\n",
       "      <td>45.43,3</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ecublens VD</td>\n",
       "      <td>21-H20</td>\n",
       "      <td>484.</td>\n",
       "      <td>2:03.37,1</td>\n",
       "      <td>1994</td>\n",
       "      <td>Abadie Théo</td>\n",
       "      <td>(3616)</td>\n",
       "      <td>53.40,3</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>St-Légier-La Chiésaz</td>\n",
       "      <td>21-H50</td>\n",
       "      <td>139.</td>\n",
       "      <td>1:45.25,0</td>\n",
       "      <td>1966</td>\n",
       "      <td>Abaidia Jilani</td>\n",
       "      <td>(4078)</td>\n",
       "      <td>23.51,0</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>St-Légier</td>\n",
       "      <td>21-D40</td>\n",
       "      <td>77.</td>\n",
       "      <td>1:53.23,1</td>\n",
       "      <td>1972</td>\n",
       "      <td>Abaidia Sandrine</td>\n",
       "      <td>(4076)</td>\n",
       "      <td>30.40,6</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Renens VD</td>\n",
       "      <td>10-H20</td>\n",
       "      <td>428.</td>\n",
       "      <td>53.23,8</td>\n",
       "      <td>1991</td>\n",
       "      <td>Abawi Khaled</td>\n",
       "      <td>(12280)</td>\n",
       "      <td>23.59,5</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     an   retard                 nom     doss                  lieu  \\\n",
       "0  1981  45.43,3  Aarskog Kay-Morten  (16798)              N-Oteren   \n",
       "1  1994  53.40,3         Abadie Théo   (3616)           Ecublens VD   \n",
       "2  1966  23.51,0      Abaidia Jilani   (4078)  St-Légier-La Chiésaz   \n",
       "3  1972  30.40,6    Abaidia Sandrine   (4076)             St-Légier   \n",
       "4  1991  23.59,5        Abawi Khaled  (12280)             Renens VD   \n",
       "\n",
       "       temps  rang  race-year distance  gender  \n",
       "0  1:15.48,7  676.       2017       10    male  \n",
       "1  2:03.37,1  484.       2017       21    male  \n",
       "2  1:45.25,0  139.       2017       21    male  \n",
       "3  1:53.23,1   77.       2017       21  female  \n",
       "4    53.23,8  428.       2017       10    male  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print overview of columns for inspection\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save dataframe to csv\n",
    "df.to_csv('../data/marathon-data.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada]",
   "language": "python",
   "name": "conda-env-ada-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
