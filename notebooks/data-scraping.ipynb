{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "% matplotlib inline\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings_p = '../data/rankings.p'\n",
    "headers_p = '../data/headers.p'\n",
    "\n",
    "# function to scrape marathon data\n",
    "def scrape_marathon_data():\n",
    "    years = range(2002, 2018)\n",
    "    letters = [chr(x + ord('A')) for x in range(0, 26)]\n",
    "\n",
    "    URL = \"https://services.datasport.com/{}/lauf/lamara/ALFA{}.HTM\"\n",
    "    \n",
    "    rankings = {}\n",
    "    headers = {}\n",
    "    for year in years:\n",
    "        print(str(year) + \": \", end='')\n",
    "        rankings[str(year)] = []\n",
    "        for letter in letters:\n",
    "            r = requests.get(URL.format(year, letter))\n",
    "            soup = bs(r.text, 'html.parser')\n",
    "            title = soup.find('title').getText()\n",
    "            if(title != 'Adresse nicht vorhanden / The address is not available'):\n",
    "                data = soup.findAll('font', {'size':'2'})\n",
    "\n",
    "                print(letter, end='')\n",
    "                headers[str(year)] = data[0].getText()\n",
    "                rankings[str(year)].extend(re.compile('¦ *\\r\\n').split(data[1].getText()))\n",
    "            else:\n",
    "                print(\"Skipped\")\n",
    "        print('')\n",
    "\n",
    "    # Pickle data\n",
    "    pickle.dump(rankings, open(rankings_p, 'wb'))\n",
    "    pickle.dump(headers, open(headers_p, 'wb'))\n",
    "    return rankings, headers\n",
    "\n",
    "# Only scrape if pickle unavailable\n",
    "def get_data(force_scrape=False):\n",
    "    if force_scrape:\n",
    "        print('Scrape data from website')\n",
    "        return scrape_marathon_data()\n",
    "    try:\n",
    "        print('Trying to load data from pickle')\n",
    "        return pickle.load(open(rankings_p, 'rb')), pickle.load(open(headers_p, 'rb'))\n",
    "    except (OSError, IOError) as e:\n",
    "        print('Failed to load pickle:')\n",
    "        print(e)\n",
    "        print('Scrape data from website')\n",
    "        return scrape_marathon_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function finds the position of where the pages should be split into columns\n",
    "based on whitespace\n",
    "\"\"\"\n",
    "def parse_columns(ranking, header):\n",
    "    column_structure = {}\n",
    "    column_mask = np.ones(np.array(list(ranking[0])).shape, dtype=bool)\n",
    "    spaces_mask = np.ones(np.array(list(ranking[0])).shape)*32\n",
    "    \n",
    "    # create mask of space positions for field delimiter detection\n",
    "    for line in ranking[:-1]:\n",
    "        cur_line = np.array(list(line)).view(np.uint32)\n",
    "        \n",
    "        try:\n",
    "            column_mask = np.logical_and(column_mask, np.equal(cur_line, spaces_mask))\n",
    "        except:\n",
    "            #print(\"Skipped line:\", line)\n",
    "            #print(cur_line.shape, column_mask.shape)\n",
    "            pass\n",
    "        \n",
    "    # include header in space position mask\n",
    "    header_int = np.array(list(header[:column_mask.shape[0]])).view(np.uint32)\n",
    "    column_mask = np.logical_and(column_mask, np.equal(header_int, spaces_mask))\n",
    "        \n",
    "    # find field delimiter positions\n",
    "    inside_space_col = False\n",
    "    delimiters = []\n",
    "    for i, is_space_col in enumerate(list(column_mask)):\n",
    "        if is_space_col:\n",
    "            if not inside_space_col:\n",
    "                inside_space_col = True\n",
    "                delimiters.append(i)\n",
    "                \n",
    "        elif inside_space_col:\n",
    "            inside_space_col = False\n",
    "            \n",
    "    return delimiters\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "Seperates the lists of lines into columns and create a dictionary of dataframes\n",
    "\"\"\"\n",
    "def separate_cols(rankings, headers):\n",
    "    # dictionary to keep dataframes in\n",
    "    dfs = {}\n",
    "    for year in rankings:\n",
    "        ranking = rankings[year]\n",
    "        header = headers[year]\n",
    "        \n",
    "        # fix offset headers in 2007-2009 data\n",
    "        if year in ['2007', '2008', '2009']:\n",
    "            header = header.replace('an lieu', 'an   lieu')\n",
    "        delimiters = parse_columns(ranking, header)\n",
    "\n",
    "        # find positions for each field\n",
    "        start = 0\n",
    "        cols = {} \n",
    "        unnamed = 0\n",
    "        for delimiter in delimiters:\n",
    "            key = header[start:delimiter+1].strip()\n",
    "            if key == '':\n",
    "                key = unnamed\n",
    "                unnamed += 1\n",
    "            cols[key] = (start, delimiter+1)\n",
    "            start = delimiter+1\n",
    "\n",
    "        start = 0\n",
    "        result = {}\n",
    "        for i, line in enumerate(ranking):\n",
    "            if '\\r\\ntotal ' not in line and '\\r\\nTotal' not in line:\n",
    "                for col in cols:\n",
    "                    start, end = cols[col]\n",
    "                    if col in result:\n",
    "                        result[col].append(line[start:end].strip())\n",
    "                    else:\n",
    "                        result[col] = [line[start:end].strip()]\n",
    "        dfs[year] = pd.DataFrame(result, columns=result.keys()) \n",
    "    print('Columns separated')\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    'lieu': '',\n",
    "    0: 'DELETE',\n",
    "    2: 'DELETE',\n",
    "    3: 'DELETE',\n",
    "    'catégorie': '',\n",
    "    'nom': '',\n",
    "    1: 'DELETE',\n",
    "    'temps': '',\n",
    "    'Jg': 'an',\n",
    "    'an lieu': '',\n",
    "    'overall': '',\n",
    "    'retard': '',\n",
    "    'équipe': '',\n",
    "    'moyenne': '',\n",
    "    'Rang': 'rang',\n",
    "    'nom/lieu': '',\n",
    "    'Stnr': 'doss',\n",
    "    'an': '',\n",
    "    'pénalité': '',\n",
    "    'doss': '',\n",
    "    'temps net': '',\n",
    "    'rang': '',\n",
    "    '¦': 'DELETE',\n",
    "    'équipe/lieu': '',\n",
    "    'Rückstand': 'retard',\n",
    "    'Kategorie': 'catégorie',\n",
    "    'Team/Ortschaft': 'équipe/lieu',\n",
    "    'Name/Ort': 'nom/lieu',\n",
    "    'Zeit': 'temps'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Where possible, this function matches the names of the columns \n",
    "of the individual dataframes\n",
    "\"\"\"\n",
    "def normalize_column_names(dfs_):\n",
    "    dfs_c = dfs_.copy()\n",
    "    for year, _ in dfs_c.items():\n",
    "        for header_name in list(dfs_c[year]):\n",
    "            if mapping[header_name] == 'DELETE':\n",
    "                dfs_c[year] = dfs_c[year].drop(header_name, axis=1)\n",
    "            elif mapping[header_name] != \"\":\n",
    "                dfs_c[year] = dfs_c[year].rename(columns = {header_name : mapping[header_name]})\n",
    "                print(\"Renamed {} to {} in year {}\".format(\n",
    "                    header_name, mapping[header_name], year))\n",
    "    print('Column names normalized')\n",
    "    return dfs_c\n",
    "                \n",
    "\n",
    "\n",
    "def print_col_overview(dfs):\n",
    "    # fields present in all years\n",
    "    shared_fields = set.intersection(*[set(v) for k, v in dfs.items()])\n",
    "    print(\"Shared fields:\", shared_fields)\n",
    "\n",
    "    fields = set()\n",
    "    for year in sorted(dfs):\n",
    "        fields = fields.union(set(list(dfs[year])))\n",
    "        print(year)\n",
    "        print(set(dfs[year]).difference(shared_fields))\n",
    "    print(\"Union remaining fields:\", fields.difference(shared_fields))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fix_cities(dfs_):\n",
    "    dfs_c = dfs_.copy()\n",
    "    \"\"\"\n",
    "    Years 2002-2006 have a team/city field and a name/city field.\n",
    "    If a team is provided, the city is added to the end of the name,\n",
    "    after a comma (',').\n",
    "    \n",
    "    This function removes the teams and converts the column 'équipe/lieu' \n",
    "    to a dedicated 'lieu' column\n",
    "    \"\"\"\n",
    "    for year, df in dfs_c.items():\n",
    "        col_name = 'nom/lieu'\n",
    "        col_team = 'équipe/lieu'\n",
    "        if col_name in df:\n",
    "            for i, row in df.iterrows():\n",
    "                m = re.compile('(.*), (.*)').match(row[col_name])\n",
    "                if m is not None:\n",
    "                    dfs_c[year].at[i, col_name] = m.group(1)\n",
    "                    dfs_c[year].at[i, col_team] = m.group(2)\n",
    "            # rename columns\n",
    "        rename_dict = {col_name : 'nom', col_team: 'lieu'}\n",
    "        dfs_c[year].rename(columns=rename_dict, inplace=True)\n",
    "    print('Cities fixed')\n",
    "    return dfs_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Returns the fields that are present in all years\n",
    "\"\"\"\n",
    "def get_shared_fields(dfs):\n",
    "    return set.intersection(*[set(v) for k, v in dfs.items()])\n",
    "\n",
    "\"\"\"\n",
    "Returns the fields that are only present in some of the years\n",
    "\"\"\"\n",
    "def get_unshared_fields(dfs):\n",
    "    return set.union(*[set(v) for k, v in dfs.items()]).difference(get_shared_fields(dfs))\n",
    "\n",
    "\"\"\"\n",
    "Deletes columns from dataframes that are only present in some of the years\n",
    "\"\"\"\n",
    "def drop_uncommon_fields(dfs_):\n",
    "    dfs_c = dfs_.copy()\n",
    "    for year, df in dfs_c.items():\n",
    "        dfs_c[year] = dfs_c[year][list(get_shared_fields(dfs_c))]\n",
    "    print('Uncommon fields dropped')\n",
    "    return dfs_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_year_column(dfs_):\n",
    "    dfs_c = dfs_.copy()\n",
    "    for year in dfs_c:\n",
    "        df = dfs_c[year].copy()\n",
    "        df['race-year'] = int(year)\n",
    "        dfs_c[year] = df\n",
    "    print('Year column created')\n",
    "    return dfs_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_merged_df(dfs_):\n",
    "    return pd.concat(list(dfs_.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_categories(df_):\n",
    "    \"\"\"\n",
    "    rename important categories and drop unused\n",
    "    \"\"\"\n",
    "    df_c = df_.copy()\n",
    "    df_c['catégorie'] = df_['catégorie'].replace({\n",
    "        r'^(\\d\\d)[-/][MQS]*([DH])\\d+$' : r'\\1|\\2',\n",
    "        r'^Q([DH])\\d$' : r'10|\\1',\n",
    "        r'^S([DH])\\d$' : r'21|\\1',\n",
    "        r'^M([DH])\\d$' : r'42|\\1'\n",
    "    }, regex = True)\n",
    "    \n",
    "    # columns to keep\n",
    "    keep = [str(x) + '|H' for x in [10, 21, 42]]\n",
    "    keep.extend([str(x) + '|D' for x in [10, 21, 42]])\n",
    "    \n",
    "    # drop all other rows\n",
    "    df_c = df_c[df_c['catégorie'].isin(keep)]\n",
    "    \n",
    "    # split catégorie into two columns\n",
    "    df_c[['distance', 'gender']] = df_c['catégorie'].str.split(\n",
    "        '|', expand=True\n",
    "    )\n",
    "    \n",
    "    # replace german gender abbreviations\n",
    "    df_c['gender'] = df_c['gender'].replace({\n",
    "        'H' : 'male',\n",
    "        'D' : 'female'\n",
    "    })\n",
    "    \n",
    "    df_c = df_c.drop('catégorie', axis=1)\n",
    "    \n",
    "    return df_c\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load data from pickle\n",
      "Columns separated\n",
      "Renamed Zeit to temps in year 2002\n",
      "Renamed Name/Ort to nom/lieu in year 2002\n",
      "Renamed Jg to an in year 2002\n",
      "Renamed Kategorie to catégorie in year 2002\n",
      "Renamed Rückstand to retard in year 2002\n",
      "Renamed Rang to rang in year 2002\n",
      "Renamed Team/Ortschaft to équipe/lieu in year 2002\n",
      "Renamed Stnr to doss in year 2002\n",
      "Column names normalized\n",
      "Cities fixed\n",
      "Uncommon fields dropped\n",
      "Year column created\n"
     ]
    }
   ],
   "source": [
    "# Get the data\n",
    "rankings, headers = get_data()\n",
    "# Separate data into columns and build dataframes\n",
    "dfs = separate_cols(rankings, headers)\n",
    "# Make sure the same columns have the same column names\n",
    "dfs_n = normalize_column_names(dfs)\n",
    "# Fix cities which are in the name column in some cases\n",
    "dfs_f = fix_cities(dfs_n)\n",
    "# Drop columns that are unique to some years\n",
    "dfs_d = drop_uncommon_fields(dfs_f)\n",
    "# Create year columns\n",
    "dfs_y = create_year_column(dfs_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create final dataframe\n",
    "df = create_merged_df(dfs_y)\n",
    "# normalize categories\n",
    "df = normalize_categories(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nom</th>\n",
       "      <th>an</th>\n",
       "      <th>lieu</th>\n",
       "      <th>doss</th>\n",
       "      <th>rang</th>\n",
       "      <th>temps</th>\n",
       "      <th>retard</th>\n",
       "      <th>race-year</th>\n",
       "      <th>distance</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abantes Jose</td>\n",
       "      <td>55</td>\n",
       "      <td>Sierre</td>\n",
       "      <td>(3001)</td>\n",
       "      <td>1.</td>\n",
       "      <td>1:09.47,4</td>\n",
       "      <td>-----</td>\n",
       "      <td>2003</td>\n",
       "      <td>21</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abbiati Franco</td>\n",
       "      <td>65</td>\n",
       "      <td>Umiken</td>\n",
       "      <td>(1965)</td>\n",
       "      <td>474.</td>\n",
       "      <td>4:56.06,0</td>\n",
       "      <td>2:42.53,7</td>\n",
       "      <td>2003</td>\n",
       "      <td>42</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abcottspon Julia</td>\n",
       "      <td>50</td>\n",
       "      <td>Gwatt (Thun)</td>\n",
       "      <td>(3002)</td>\n",
       "      <td>4.</td>\n",
       "      <td>1:37.53,9</td>\n",
       "      <td>11.53,9</td>\n",
       "      <td>2003</td>\n",
       "      <td>21</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abegglen Eddy</td>\n",
       "      <td>54</td>\n",
       "      <td>Mürren</td>\n",
       "      <td>(1940)</td>\n",
       "      <td>432.</td>\n",
       "      <td>4:24.11,4</td>\n",
       "      <td>1:47.58,3</td>\n",
       "      <td>2003</td>\n",
       "      <td>42</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abel Marit</td>\n",
       "      <td>55</td>\n",
       "      <td>N-Hamar</td>\n",
       "      <td>(7001)</td>\n",
       "      <td>112.</td>\n",
       "      <td>58.03,1</td>\n",
       "      <td>16.32,2</td>\n",
       "      <td>2003</td>\n",
       "      <td>10</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                nom  an          lieu    doss  rang      temps     retard  \\\n",
       "0      Abantes Jose  55        Sierre  (3001)    1.  1:09.47,4      -----   \n",
       "1    Abbiati Franco  65        Umiken  (1965)  474.  4:56.06,0  2:42.53,7   \n",
       "2  Abcottspon Julia  50  Gwatt (Thun)  (3002)    4.  1:37.53,9    11.53,9   \n",
       "3     Abegglen Eddy  54        Mürren  (1940)  432.  4:24.11,4  1:47.58,3   \n",
       "4        Abel Marit  55       N-Hamar  (7001)  112.    58.03,1    16.32,2   \n",
       "\n",
       "   race-year distance  gender  \n",
       "0       2003       21    male  \n",
       "1       2003       42    male  \n",
       "2       2003       21  female  \n",
       "3       2003       42    male  \n",
       "4       2003       10  female  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print overview of columns for inspection\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save dataframe to csv\n",
    "df.to_csv('../data/marathon-data.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_loc = pd.read_csv('../data/marathon-data-with-geo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cols = df_loc[[\n",
    "    'lieu', 'format_city', 'Canton', \n",
    "    'country', 'lat', 'lng'\n",
    "]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(df, merge_cols, on='lieu', how='inner')\n",
    "df_merged = df_merged.dropna(subset=['country'])\n",
    "df_merged.to_csv('../data/marathon-data-gender-race-l')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada]",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
